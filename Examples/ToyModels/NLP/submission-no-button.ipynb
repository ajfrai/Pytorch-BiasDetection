{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SuccessfulTest.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKn9q33et32Y",
        "colab_type": "text"
      },
      "source": [
        "# Bias Test Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckke7M7kuzlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torch.utils.data import dataset\n",
        "import torch.nn as nn\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5jt_uOWurSv",
        "colab_type": "text"
      },
      "source": [
        "## Load the Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUtR8jJdvAPn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5da7df45-a3df-435b-e2bd-6b3da18470d3"
      },
      "source": [
        "weights_in_dir = ('saved_weights.pt' in os.listdir())\n",
        "if not weights_in_dir:\n",
        "  print(\"Please add your model as saved_weights.pt to the current working directory before proceeding.\")\n",
        "else:\n",
        "  print(\"saved_weights.pt is in the current working directory. You may proceed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved_weights.pt is in the current working directory. You may proceed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsb13-MMutU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path= 'saved_weights.pt'\n",
        "weights = torch.load(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9-t5Cn-uCLC",
        "colab_type": "text"
      },
      "source": [
        "## Define and instantiate the model\n",
        "This will be presented to the submitter as a stub. They can fill it in as they see fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-2wUme0ueFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class classifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()          \n",
        "        \n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        \n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs=self.fc(hidden)\n",
        "\n",
        "        #Final activation function\n",
        "        outputs=self.act(dense_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTgb3V_wv5eW",
        "colab_type": "text"
      },
      "source": [
        "## Set hyperparameters\n",
        "Again this will be presented to the submitter as a stub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vri8z8w_um6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def instantiate_model():\n",
        "  size_of_vocab = weights[\"embedding.weight\"].size()[0]\n",
        "  embedding_dim = 100\n",
        "  num_hidden_nodes = 32\n",
        "  num_output_nodes = 1\n",
        "  num_layers = 2\n",
        "  bidirection = True\n",
        "  dropout = 0.2\n",
        "\n",
        "  #instantiate the model\n",
        "  model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
        "                    bidirectional = True, dropout = dropout)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts1BFT89wHP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7bcbdebc-0dab-4b9e-f96a-84ae66474ce0"
      },
      "source": [
        "model = instantiate_model()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(13906, 100)\n",
            "  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (act): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuDnCSjXuFOC",
        "colab_type": "text"
      },
      "source": [
        "## Define a preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW65lhY2wulM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d97f134-9d79-443d-a35e-5b35712c01e8"
      },
      "source": [
        "# import any packages required for preprocessing \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from django.core.validators import URLValidator\n",
        "from django.core.exceptions import ValidationError"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd7tjTdmwMEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(input_string,stop_words=None,punct_strip=None):\n",
        "  def is_url(token):\n",
        "    val = URLValidator()\n",
        "    try:\n",
        "        val(token)\n",
        "        return True\n",
        "    except ValidationError:\n",
        "        return False\n",
        "\n",
        "  if stop_words == None:\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "  if punct_strip == None:\n",
        "    punct_strip = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "  # tokenize\n",
        "  input_string = input_string.split(\" \")\n",
        "  final_input_string = []\n",
        "\n",
        "  for token in input_string:\n",
        "    # lower case\n",
        "    token = token.lower()\n",
        "\n",
        "    # remove stop words, mentions, urls\n",
        "    if token and (token in stop_words or token[0] == '@' or is_url(token)):\n",
        "      continue # do not append it to final_tweet\n",
        "\n",
        "\n",
        "    # remove punctuation \n",
        "    token = token.translate(punct_strip)\n",
        "\n",
        "  \n",
        "    \n",
        "    if len(token) > 0:\n",
        "      final_input_string.append(token)\n",
        "\n",
        "  return \" \".join(final_input_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zsaUSHfw53t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd91bae5-4bab-4292-a294-7615334a32f2"
      },
      "source": [
        "#Test preprocessor \n",
        "preprocess(\"hello @mr http://hello.com hello Hello!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello hello hello'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG3gdLVzuI_7",
        "colab_type": "text"
      },
      "source": [
        "## Define a prediction function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejMh15o0t9NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "train_data=data.TabularDataset(path = '/content/drive/My Drive/PyTorchData/train.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "TEXT.build_vocab(train_data,min_freq=0,vectors = \"glove.6B.100d\")  \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "def predict(model, sentence):\n",
        "    sentence = preprocess(sentence)\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
        "    length = [len(indexed)]                                    #compute no. of words\n",
        "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
        "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
        "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
        "    prediction = model(tensor, length_tensor)                  #prediction \n",
        "    prediction = prediction[0].item()\n",
        "    ### Quantitative to Class Name\n",
        "    if prediction < .3:\n",
        "      return \"Unintelligent\"\n",
        "    elif prediction < .6:\n",
        "      return \"Moderately Intelligent\"\n",
        "    else:\n",
        "      return \"Intelligent\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNxon5KMzE-V",
        "colab_type": "text"
      },
      "source": [
        "## Define your output classes in social impact order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA7DevJgzkbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "social_impact_order = [\"Unintelligent\",\"Moderately Intelligent\",\"Intelligent\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnBr8c1vuLbU",
        "colab_type": "text"
      },
      "source": [
        "## Test to see the prediction function returns an output class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJAbDxuVt3Ej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b3e7c50e-edb9-48bc-d87d-fc858a6eac98"
      },
      "source": [
        "print(\"Running model on an example short text.\")\n",
        "result = predict(model,\"hello @hello https://hello.com what's up\") in social_impact_order\n",
        "\n",
        "print(\"Prediction class is listed in social_impact_order: \" + str(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running model on an example short text\n",
            "Prediction class is listed in social_impact_order: True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}